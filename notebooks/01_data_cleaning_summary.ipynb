{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "# Data Cleaning Summary - MovieLens 100K SVD Ready\n",
    "âœ… **à¸à¸²à¸£à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ - à¸à¸£à¹‰à¸­à¸¡à¸ªà¸³à¸«à¸£à¸±à¸š SVD Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_cleaned_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load cleaned data\n",
    "data_dir = os.path.join(\"..\", \"data\")\n",
    "final_data = pd.read_csv(os.path.join(data_dir, \"final_data_cleaned.csv\"))\n",
    "svd_data = pd.read_csv(os.path.join(data_dir, \"svd_data.csv\")) if os.path.exists(os.path.join(data_dir, \"svd_data.csv\")) else None\n",
    "\n",
    "print(\"ğŸ“Š Data Cleaning Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"âœ… Total processed ratings: {len(final_data):,}\")\n",
    "print(f\"âœ… Unique users: {final_data['user_id'].nunique():,}\")\n",
    "print(f\"âœ… Unique movies: {final_data['item_id'].nunique():,}\")\n",
    "print(f\"âœ… Rating range: {final_data['rating'].min()} - {final_data['rating'].max()}\")\n",
    "print(f\"âœ… Data sparsity: {((1 - len(final_data) / (final_data['user_id'].nunique() * final_data['item_id'].nunique())) * 100):.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Available columns:\")\n",
    "for i, col in enumerate(final_data.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_sample_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of cleaned data\n",
    "print(\"ğŸ¬ Sample of Cleaned Data:\")\n",
    "print(\"=\" * 50)\n",
    "final_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rating_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating distribution\n",
    "print(\"â­ Rating Distribution:\")\n",
    "print(\"=\" * 30)\n",
    "rating_dist = final_data['rating'].value_counts().sort_index()\n",
    "for rating, count in rating_dist.items():\n",
    "    percentage = (count / len(final_data)) * 100\n",
    "    print(f\"Rating {rating}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Genre distribution (top 10)\n",
    "print(\"\\nğŸ­ Top 10 Genres:\")\n",
    "print(\"=\" * 20)\n",
    "genre_counts = {}\n",
    "for genres_str in final_data['genres'].dropna():\n",
    "    for genre in genres_str.split(', '):\n",
    "        genre_counts[genre] = genre_counts.get(genre, 0) + 1\n",
    "\n",
    "top_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for genre, count in top_genres:\n",
    "    print(f\"{genre}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "user_demographics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User demographics\n",
    "print(\"ğŸ‘¥ User Demographics:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "print(\"Gender Distribution:\")\n",
    "gender_dist = final_data.groupby('user_id')['gender'].first().value_counts()\n",
    "for gender, count in gender_dist.items():\n",
    "    percentage = (count / len(gender_dist)) * 100\n",
    "    print(f\"  {gender}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nAge Group Distribution:\")\n",
    "age_dist = final_data.groupby('user_id')['age_group'].first().value_counts()\n",
    "for age_group, count in age_dist.items():\n",
    "    percentage = (count / len(age_dist)) * 100\n",
    "    print(f\"  {age_group}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nTop 5 Occupations:\")\n",
    "occupation_dist = final_data.groupby('user_id')['occupation'].first().value_counts().head()\n",
    "for occupation, count in occupation_dist.items():\n",
    "    percentage = (count / final_data['user_id'].nunique()) * 100\n",
    "    print(f\"  {occupation}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_quality_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"âœ… Data Quality Verification:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Missing values check\n",
    "missing_values = final_data.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "for col, missing in missing_values.items():\n",
    "    if missing > 0:\n",
    "        percentage = (missing / len(final_data)) * 100\n",
    "        print(f\"  {col}: {missing} ({percentage:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {col}: âœ… No missing values\")\n",
    "\n",
    "# Data type verification\n",
    "print(\"\\nData Types:\")\n",
    "for col in final_data.columns:\n",
    "    dtype = final_data[col].dtype\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "# Rating validation\n",
    "invalid_ratings = final_data[(final_data['rating'] < 1) | (final_data['rating'] > 5)]\n",
    "print(f\"\\nRating Validation: {'âœ… All ratings valid (1-5)' if len(invalid_ratings) == 0 else f'âŒ {len(invalid_ratings)} invalid ratings found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svd_ready_confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD readiness confirmation\n",
    "print(\"ğŸš€ SVD Model Training Readiness:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check if user-item matrix can be created\n",
    "try:\n",
    "    user_item_matrix = final_data.pivot_table(\n",
    "        index='user_id', \n",
    "        columns='item_id', \n",
    "        values='rating'\n",
    "    )\n",
    "    print(f\"âœ… User-Item Matrix: {user_item_matrix.shape}\")\n",
    "    print(f\"âœ… Matrix sparsity: {((user_item_matrix.isna().sum().sum() / user_item_matrix.size) * 100):.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ User-Item Matrix creation failed: {e}\")\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"âœ… Unique user IDs: {final_data['user_id'].nunique()}\")\n",
    "print(f\"âœ… Unique movie IDs: {final_data['item_id'].nunique()}\")\n",
    "print(f\"âœ… Total ratings: {len(final_data):,}\")\n",
    "\n",
    "# Files created\n",
    "print(\"\\nğŸ“ Files Created:\")\n",
    "files_to_check = [\n",
    "    \"final_data_cleaned.csv\",\n",
    "    \"svd_data.csv\",\n",
    "    \"user_item_matrix.csv\"\n",
    "]\n",
    "\n",
    "for filename in files_to_check:\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"  âœ… {filename} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  âŒ {filename} (missing)\")\n",
    "\n",
    "print(\"\\nğŸ¯ Next Steps:\")\n",
    "print(\"1. Proceed to notebook 02_exploratory_analysis.ipynb\")\n",
    "print(\"2. Then move to 03_model_svd.ipynb for SVD training\")\n",
    "print(\"3. Data is ready for collaborative filtering algorithms!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
